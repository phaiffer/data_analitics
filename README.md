## ğŸ“ DocumentaÃ§Ã£o â€“ ExtraÃ§Ã£o de Perfis da Web Summit

### ğŸ“Œ Objetivo

Este script automatiza a extraÃ§Ã£o de informaÃ§Ãµes pÃºblicas de startups listadas na Web Summit, acessando suas pÃ¡ginas individuais e coletando:

* Nome da startup
* DescriÃ§Ã£o
* PaÃ­s
* Segmento
* Link para Instagram
* Link para LinkedIn
* Website oficial

Os dados sÃ£o extraÃ­dos usando `Selenium` + `BeautifulSoup` e salvos em um arquivo `.xlsx`.

---

### ğŸ“‚ Entrada

Um arquivo CSV chamado `startup_urls.csv` com a seguinte estrutura:

```csv
url
https://websummit.com/appearances/lis24/xxxxxx/startup-name
https://websummit.com/appearances/lis24/yyyyyy/startup-name-2
...
```

A coluna `url` deve conter os links diretos das startups no site da Web Summit.

---

### ğŸ“¦ Requisitos

Instale os pacotes necessÃ¡rios:

```bash
pip install selenium beautifulsoup4 pandas chromedriver-autoinstaller openpyxl
```

---

### ğŸ§  Como Funciona

1. **LÃª as URLs do arquivo `startup_urls.csv`**
2. **Abre cada pÃ¡gina usando Selenium (com Chrome)**
3. **Aguarda atÃ© que os dados estejam carregados**
4. **Usa BeautifulSoup para fazer parsing do HTML**
5. **Extrai os dados usando seletores CSS**
6. **Filtra os links do Instagram e LinkedIn**
7. **Coleta o link do website (posiÃ§Ã£o 2 entre os botÃµes)**
8. **Armazena todos os dados em uma planilha Excel (`.xlsx`)**

---

### ğŸ“¤ SaÃ­da

* `perfis_extraidos.xlsx`: Planilha com os dados extraÃ­dos
* `erros_de_extracao.xlsx` (opcional): URLs que apresentaram falha na coleta

---

### ğŸ§¾ Estrutura do CÃ³digo

#### Bibliotecas e configuraÃ§Ã£o do WebDriver:

* Usa `chromedriver_autoinstaller` para garantir compatibilidade com o navegador.
* `Selenium` Ã© usado para renderizar a pÃ¡gina e esperar o conteÃºdo carregar.

#### Leitura de URLs:

```python
df_urls = pd.read_csv('startup_urls.csv')
```

#### ExtraÃ§Ã£o de informaÃ§Ãµes com `BeautifulSoup`:

```python
soup.select('[class^="headlines__H1"]')  # Nome  
soup.select('[class^="ProfileDetails__ProfileDetailsContent"] > div')  # DescriÃ§Ã£o  
soup.select('[class^="ContentTagList__ContentTagListItem"]')  # PaÃ­s e Segmento  
soup.select('[class^="DetailsLayout__SocialMediaWrapper"]')  # Redes sociais  
soup.select('[class^="Button__StyledButton"]')  # BotÃµes (Website)
```

#### Filtro de links:

* Instagram: se `'instagram.com' in href`
* LinkedIn: se `'linkedin.com' in href`
* Website: botÃ£o na posiÃ§Ã£o 2 (Ã­ndice 2) da lista de botÃµes com `href`

---

### ğŸ“Œ Exemplo de saÃ­da:

| name     | description    | country | segment | instagram                                           | linkedin                                          | web\_site                                  |
| -------- | -------------- | ------- | ------- | --------------------------------------------------- | ------------------------------------------------- | ------------------------------------------ |
| Schoolux | Edtech startup | Brazil  | EdTech  | [https://instagram.com/](https://instagram.com/)... | [https://linkedin.com/](https://linkedin.com/)... | [https://schoolux.ai](https://schoolux.ai) |

---

### ğŸ›  PossÃ­veis melhorias (futuras)

* Adicionar `time.sleep()` aleatÃ³rio entre requisiÃ§Ãµes para evitar bloqueios.
* Permitir salvar dados parcialmente mesmo se interrompido.
* Usar `tqdm` para barra de progresso.
* Modularizar em funÃ§Ãµes ou classes para reaproveitamento.

---

## ğŸ“§ DocumentaÃ§Ã£o â€“ Coletor de E-mails de Sites

### ğŸ“Œ Objetivo

Este script automatiza a extraÃ§Ã£o de endereÃ§os de e-mail de sites informados em um arquivo `.csv`. Ele navega nas pÃ¡ginas iniciais e tambÃ©m em pÃ¡ginas relacionadas a contato, utilizando `Selenium` e `re` (expressÃµes regulares).

---

### ğŸ“‚ Entrada

Um arquivo CSV chamado `links.csv` com uma coluna `url`:

```csv
url
https://empresa1.com
https://empresa2.org
...
```

---

### ğŸ“¤ SaÃ­da

* `emails_encontrados.csv`: ContÃ©m os sites e os e-mails encontrados.
* `sem_email.csv`: Lista os sites onde nenhum e-mail foi encontrado.

---

### ğŸ“¦ Requisitos

```bash
pip install selenium pandas chromedriver-autoinstaller
```

---

### ğŸ§  Como Funciona

1. LÃª os sites a partir de `links.csv`.
2. Usa `Selenium` (em modo headless) para visitar os sites.
3. Extrai os e-mails da pÃ¡gina inicial.
4. Se necessÃ¡rio, acessa pÃ¡ginas de contato (usando palavras-chave como â€œcontatoâ€, â€œcontactâ€, etc.).
5. Extrai e valida os e-mails usando expressÃµes regulares.
6. Salva os resultados em dois arquivos CSV.

---

### ğŸ” Palavras-chave para identificar pÃ¡ginas de contato

```python
KEYWORDS = ['contato', 'fale conosco', 'about', 'about us', 'contact', 'contact us', 'contÃ¡ctanos']
```

---

### ğŸ“ Principais FunÃ§Ãµes

#### `get_domain_from_url(url)`

Extrai e padroniza o domÃ­nio a partir da URL.

#### `create_email_pattern(domain)`

Gera um padrÃ£o regex especÃ­fico para e-mails daquele domÃ­nio.

#### `setup_driver(headless)`

Configura o navegador Chrome para automaÃ§Ã£o.

#### `find_emails_on_page(driver, pattern)`

Busca e-mails na pÃ¡gina atual com base no padrÃ£o regex.

#### `find_contact_links(driver)`

Procura links na pÃ¡gina atual que levem a pÃ¡ginas de contato.

#### `scrape_emails_from_site(url)`

FunÃ§Ã£o principal que executa o processo de scraping em um Ãºnico site.

#### `save_results(...)`

Salva os e-mails encontrados e os sites sem e-mail nos arquivos finais.

---

### ğŸ›  SugestÃµes de melhorias

* Adicionar suporte a mÃºltiplos domÃ­nios por pÃ¡gina.
* Tornar o tempo de `sleep` aleatÃ³rio para evitar bloqueios.
* Adicionar tratamento para redirecionamentos.
* Incluir barra de progresso com `tqdm`.

---

### ğŸ‘¨â€ğŸ’» Autor

Willian Phaiffer Cardoso.

---
